{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d1455a-fff7-4ba6-aa9f-c7d5ef79de5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CustomAugment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfacenet_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InceptionResnetV1\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCustomAugment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomFlip, RandomRotation, RandomBrightnessContrast, Normalize\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Added imports for visualization\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'CustomAugment'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "from CustomAugment import RandomFlip, RandomRotation, RandomBrightnessContrast, Normalize\n",
    "import mediapipe as mp\n",
    "\n",
    "# Added imports for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "weights_path = 'D:/Code Files/_models/facenet-vggface2.pt'  # Replace with the correct path to your downloaded weights file\n",
    "# print(resnet.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967ab1d-314b-4cbd-bdbc-d12c34ff9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognition:\n",
    "\n",
    "    def __init__(self, wp='./models/facenet-vggface2.pt'):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.face_detection = mp.solutions.face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.6)\n",
    "        self.weights_path = wp\n",
    "\n",
    "    def elapsedTime(self, start_time):\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"\\nElapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    def collate_np(self, batch, class_to_idx):\n",
    "        images, labels = zip(*batch)\n",
    "        target_size = (160, 160)\n",
    "        images_np = [cv2.resize(np.array(img), target_size) for img in images]\n",
    "        labels_str = [list(class_to_idx.keys())[list(class_to_idx.values()).index(label)] for label in labels]\n",
    "        return images_np, labels_str\n",
    "\n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f'Running on device: {self.device}')\n",
    "\n",
    "        resnet = InceptionResnetV1()\n",
    "\n",
    "        state_dict = torch.load(self.weights_path)\n",
    "        filtered_state_dict = OrderedDict((k, v) for k, v in state_dict.items() if k not in ['logits.weight', 'logits.bias'])\n",
    "        resnet.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "        resnet.eval()\n",
    "\n",
    "        path = './dataset/videos_cropped'\n",
    "\n",
    "        data_transforms = transforms.Compose([\n",
    "            RandomRotation(angles=(-10, -5, 0, 5, 10)),\n",
    "            RandomFlip(p=0.5),\n",
    "            RandomBrightnessContrast(brightness_range=(0.8, 1.5), contrast_range=(0.6, 1.5)),\n",
    "            transforms.Resize((160, 160)),\n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            dataset = datasets.ImageFolder(path, transform=data_transforms)\n",
    "        else:\n",
    "            print(\"Path doesn't exist\")\n",
    "            return\n",
    "\n",
    "        idx_to_class = {i: c for c, i in dataset.class_to_idx.items()}\n",
    "\n",
    "        workers = 0 if os.name == 'nt' else 4\n",
    "        loader = DataLoader(dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            num_workers=workers,\n",
    "                            collate_fn=lambda batch: self.collate_np(batch, dataset.class_to_idx)\n",
    "                            )\n",
    "\n",
    "        face_list = []  # list of cropped faces from photos folder\n",
    "        name_list = []  # list of names corrospoing to cropped photos\n",
    "\n",
    "        self.elapsedTime(start_time)\n",
    "        for batch_idx, (inputs, labels) in enumerate(loader):\n",
    "            # print(idx_to_class[idx[0]])\n",
    "            print(\"Batch #: \" + str(batch_idx))\n",
    "\n",
    "            batch_size = len(inputs)  # Get the current batch size (may be smaller for the last batch)\n",
    "            for j in range(batch_size):\n",
    "                img = inputs[j]  # Get the j-th image in the batch\n",
    "                label = labels[j]  # Get the j-th label in the batch\n",
    "\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                try:\n",
    "                    results = self.face_detection.process(img)\n",
    "                    # face, prob = mtcnn(img, return_prob=True)\n",
    "\n",
    "                    if results.detections:\n",
    "                        # Get the first detected face\n",
    "                        first_face = results.detections[0]\n",
    "\n",
    "                        bbox = first_face.location_data.relative_bounding_box\n",
    "                        x, y, w, h = int(bbox.xmin * img.shape[1]), int(bbox.ymin * img.shape[0]), \\\n",
    "                                     int(bbox.width * img.shape[1]), int(bbox.height * img.shape[0])\n",
    "\n",
    "                        # Add padding to the image\n",
    "                        border = max(w, h)  # Use the maximum of width and height as the border size\n",
    "                        img_padded = cv2.copyMakeBorder(img, border, border, border, border, cv2.BORDER_CONSTANT,\n",
    "                                                        value=[0, 0, 0])\n",
    "\n",
    "                        # Crop the face region from the padded image\n",
    "                        face_image = img_padded[y + border:y + h + border, x + border:x + w + border]\n",
    "\n",
    "                        # Convert the cropped face from OpenCV's BGR format to RGB format\n",
    "                        face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "                        # Convert the NumPy array to a PyTorch tensor\n",
    "                        face_image = torch.from_numpy(face_image).permute(2, 0, 1).float()\n",
    "\n",
    "                        # print(f'Face detected with probability: {prob.item():.4f}')\n",
    "                        face_list.append(face_image)\n",
    "                        name_list.append(label)  # names are stored in a list\n",
    "                        # elapsedTime(start_time)\n",
    "                        # print(idx_to_class[label])\n",
    "                        print(str(j), end='')\n",
    "                    else:\n",
    "                        print('.', end='')\n",
    "                        # print(idx_to_class[label])\n",
    "                        # print(\"No Face detected!\")\n",
    "                        # pass\n",
    "                except ValueError:\n",
    "                    print(\"Error detection!\")\n",
    "                    continue\n",
    "\n",
    "                # Perform garbage collection to free up memory\n",
    "                del img\n",
    "                gc.collect()\n",
    "\n",
    "            self.elapsedTime(start_time)\n",
    "\n",
    "        # ...\n",
    "\n",
    "        print(\"Face detection complete...\")\n",
    "        self.elapsedTime(start_time)\n",
    "\n",
    "        face_list_resized = []\n",
    "        for face in face_list:\n",
    "            face_np = face.numpy().transpose((1, 2, 0))  # Convert tensor to NumPy array and change channel order\n",
    "            face_pil = Image.fromarray(face_np.astype('uint8'))  # Convert NumPy array to PIL image\n",
    "            face_pil_resized = face_pil.resize((160, 160), Image.ANTIALIAS)  # Resize the PIL image\n",
    "            face_resized = transforms.ToTensor()(face_pil_resized)  # Convert the resized PIL image back to a tensor\n",
    "            face_list_resized.append(face_resized)\n",
    "\n",
    "        # print(face_list)\n",
    "        if len(face_list_resized) > 0:\n",
    "            aligned = torch.stack(face_list_resized)\n",
    "            # Continue with the rest of the code\n",
    "        else:\n",
    "            print(\"No faces were found in the dataset. Please check the input images.\")\n",
    "            return\n",
    "\n",
    "        names = np.array(name_list)\n",
    "\n",
    "        # Disable gradient calculation during inference\n",
    "        with torch.no_grad():\n",
    "            # Calculate the embeddings for the aligned faces\n",
    "            embeddings = resnet(aligned).detach().cpu()\n",
    "\n",
    "        # Encode the labels\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(names)\n",
    "\n",
    "        print(\"Training SVM...\")\n",
    "        self.elapsedTime(start_time)\n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train an SVM classifier\n",
    "        clf = SVC(kernel='linear', probability=True)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Save the trained model and the name list as a pickle file\n",
    "        with open(\"svm_classifier.pkl\", \"wb\") as f:\n",
    "            pickle.dump((clf, le, name_list), f)\n",
    "\n",
    "        # Load the trained model and the name list from the pickle file\n",
    "        with open(\"svm_classifier.pkl\", \"rb\") as f:\n",
    "            loaded_clf, loaded_le, loaded_name_list = pickle.load(f)\n",
    "\n",
    "        # Evaluate the classifier\n",
    "        y_pred = loaded_clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        self.elapsedTime(start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7005b3-ba7b-4ff5-84bc-15c180e346a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    face_recognition = FaceRecognition(weights_path)\n",
    "    face_recognition.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af093c-c237-4c0b-8725-9a252e9117bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Visualize dataset distribution\n",
    "    unique, counts = np.unique(face_recognition.name_list, return_counts=True)\n",
    "    plt.figure()\n",
    "    plt.bar(unique, counts)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab89f4-176b-4c83-ba6f-479f4abdd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Visualize confusion matrix\n",
    "    cm = confusion_matrix(face_recognition.y_test, face_recognition.y_pred)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=face_recognition.loaded_le.classes_,\n",
    "                yticklabels=face_recognition.loaded_le.classes_)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e92baa-fa28-4fbc-b103-fba88084aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification report\n",
    "    print(classification_report(face_recognition.y_test, face_recognition.y_pred, target_names=face_recognition.loaded_le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf340ba9-d36b-49b4-87fd-ec6dba4f5e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
